{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"Asia/Dubai\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"71f2594d-0153-4edf-bddb-0244d149f1f8\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"COURSERA GP: Snowflake for Data Science: Intro to Snowpark ML for Python_learner Guide_Final\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"# TASK 2 : Set-up your Snowpark for Python workspace\n- In this Task, we will walk through a how to connect HEX notebook to your Snowflake Trial Account.","metadata":{}},{"cell_type":"markdown","source":"#### _**a-Import Libraries**_\n\n","metadata":{}},{"cell_type":"code","source":"# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType\n\n# Snowpark ML\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\n\n# Data Science Libs\nimport numpy as np\nimport pandas as pd\n\n# Misc\nimport joblib\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _**b- Establish Secure Connection to Snowflake**_\n\n","metadata":{}},{"cell_type":"code","source":"# Verify connectivity to Snowflake\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 3 : Data Preprocessing: Transform categorical variables\n- In this Task, we will walk through data transformations that are included in the Snowpark ML Preprocessing API.\n\n*We will illustrate a few transformation functions availabe in Snowpark ML, the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing).*","metadata":{}},{"cell_type":"markdown","source":"#### _a- Data Loading_\n\n","metadata":{}},{"cell_type":"code","source":"# Specify the table name where we stored the diamonds dataset\n# ** ONLY Change this only if you named your table something else in the data ingest notebook **\nDIAMONDS_TABLE = 'diamonds'\ninput_tbl = f\"{session.get_current_database()}.{session.get_current_schema()}.{DIAMONDS_TABLE}\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First, we read-in the data from a Snowflake table into a Snowpark DataFrame\ndiamonds_df = \n\n# Let's visualise the Data\n\n\n# Describe Snowpark Datafarame\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Snowpark DF API into Pandas DF, then check for missing values and categorical variables in the dataset\ndiamonds_df.to_pandas().info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://4cs.gia.edu/wp-content/uploads/2012/05/Cut-Anatomy-of-Diamond.png\" style=\"height:200px\" />","metadata":{}},{"cell_type":"markdown","source":"_**Features**_\n\n- **Price :**, in US dollars ($326--$18,823)This is the target column containing tags for the features. \n\nThe 4 Cs of Diamonds:\n\n- **Carat**, ,**(0.2--5.01)**, The carat is the diamond’s physical weight measured in metric carats.  One carat equals 1/5 gram and is subdivided into 100 points. Carat weight is the most objective grade of the 4Cs. \n- _**Cut (Fair, Good, Very Good, Premium, Ideal):**_, The quality of the cut. The more precise the diamond is cut, the more captivating the diamond is to the eye thus of high grade.\n- _**Clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)):**_, Diamonds can have internal characteristics known as inclusions or external characteristics known as blemishes. Diamonds without inclusions or blemishes are rare; however, most characteristics can only be seen with magnification.\n- _**Color (from J (worst) to D (best)): **_,The color of gem-quality diamonds occurs in many hues. In the range from colorless to light yellow or light brown. Colorless diamonds are the rarest. Other natural colors (blue, red, pink for example) are known as \"fancy,” and their color grading is different from white colorless diamonds.\n- Dimensions: \n  - **X: **,length in mm (0--10.74)\n  - **Y: **,width in mm (0--58.9)\n  - **Z: **,depth in mm (0--31.8)\n\n","metadata":{}},{"cell_type":"markdown","source":"#### b-Transform `COLOR`, `CLARITY` and `CUT` from categorical to numerical values using `OrdinalEncoder`\n\n","metadata":{}},{"cell_type":"code","source":"import snowflake.ml.modeling.preprocessing as snowml\n\n# Encode CUT, CLARITY, and COLOR to preserve ordinal importance\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}\nsnowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\", \"COLOR\"], \n                                    output_cols=[\"CUT_OE\", \"CLARITY_OE\",\"COLOR_OE\"], \n                                    categories=categories\n                                )\n\nord_encoded_diamonds_df = \n\n# Show the encoding\nprint(snowml_oe._state_pandas)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show the results of the OrdinalEncoder transformer\nord_encoded_diamonds_df.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PRACTICE TASK : Normalise Numerical Column `CARAT` using `MinMaxScaler`","metadata":{}},{"cell_type":"markdown","source":"SNOWPARK ML Modeling preprocessing [Documentation](https://docs.snowflake.com/developer-guide/snowpark-ml/snowpark-ml-modeling)\n\n","metadata":{}},{"cell_type":"code","source":"from snowflake.ml.modeling.preprocessing import # >Your script goes here...<\n# hint : to get help run: \n# help(MinMaxScaler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MinMaxScaler scales the data to a fixed range, typically between 0 and 1\n\n# Normalize the CARAT column\nsnowml_mms = # >Your script goes here...<\n\n# Call : \n# a-fit(X[, y]) Compute the minimum and maximum to be used for later scaling.\n# b-transform(X) Scale features of X according to feature_range.\n\nnormalized_diamonds_df = # >Your script goes here...<\n\n# Check Results : Call .Describe() then .Show() Results\n# >Your script goes here...< ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 4 : Build Preprocessing Pipeline\n- In this Task, we will build a preprocessing pipeline to be used for both the ML training & inference steps to have standarized feature transformations.","metadata":{}},{"cell_type":"markdown","source":"#### _a- Categorise all the features for processing_\n\n","metadata":{}},{"cell_type":"code","source":"# Categorize all the features for processing\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _b- Build the pipeline_\n\n","metadata":{}},{"cell_type":"code","source":"# Build the pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n            (\n                \"OE\",\n                snowml.OrdinalEncoder(\n                    input_cols=CATEGORICAL_COLUMNS,\n                    output_cols=CATEGORICAL_COLUMNS_OE,\n                    categories=categories,\n                )\n            )\n    ]\n)\n\nPIPELINE_FILE = 'preprocessing_pipeline.joblib'\n\n# Lets create joblib file of the pipeline and save it locally\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _c- Save the pipleine into Snowflake stage as a joblib file_\n\n","metadata":{}},{"cell_type":"code","source":"# You can also save the pipeline joblibfile file into a Snowflake Internal Stage\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 5 : Build an `XGBoost` Regression Model\n- In this Task, we will illustrate how to train an XGBoost model with the diamonds dataset using the Snowpark ML Model API. \n\nThe Snowpark ML Model API currently supports sklearn, xgboost, and lightgbm models, for more details check out Snowflake Documentation : https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling ","metadata":{}},{"cell_type":"markdown","source":"#### _a- Import Libraries_\n\n","metadata":{}},{"cell_type":"code","source":"# Snowpark ML\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\n# For All Snowpark ML Modeling Classes visit https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling\n\n# joblib\nimport joblib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _b- Load the data & preprocessing pipeline_\n\n","metadata":{}},{"cell_type":"code","source":"# Load Diamonds Table from Snowflake\n#diamonds_df = session.table(input_tbl)\ndiamonds_df.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the preprocessing pipeline object\n#preprocessing_pipeline = joblib.load('preprocessing_pipeline.joblib')\n\n# Visualise the Pipeline \npreprocessing_pipeline.to_sklearn()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorize all the features for modeling\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] \nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\nLABEL_COLUMNS = ['PRICE']\nOUTPUT_COLUMNS = ['PREDICTED_PRICE']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _c- Build XGBoost Regression Model_\n\n","metadata":{}},{"cell_type":"code","source":"# Split the data into train and test sets\ndiamonds_train_df, diamonds_test_df = \n\n# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the XGBRegressor\nregressor = XGBRegressor(\n    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n\n# Train\n\n\n# Predict\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _d- Analyse Model Accuracy using Snowpark ML's R-2 Score._\n\n","metadata":{}},{"cell_type":"code","source":"from snowflake.ml.modeling.metrics import r2_score\n\n# Show Result\nresult.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n\n# Model Accuracy using R-2 Score\nprint('Acccuracy:', r2_score(df=result,y_true_col_name=\"PRICE\",y_pred_col_name=\"PREDICTED_PRICE\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _e- Plot Actual Vs Predicted _\n\n","metadata":{}},{"cell_type":"markdown","source":"# PRACTICE TASK : Try different Regression Model `LinearRegression`  ","metadata":{}},{"cell_type":"code","source":"# Import LinearRegression \nfrom snowflake.ml.modeling.linear_model import LinearRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the LinearRegression\n\n\n# Train\n\n\n# Predict\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-->** Navigate to Snowflake Query History to check how Snowflake translates and executes your Python queries \n\n","metadata":{}},{"cell_type":"code","source":"# Show Result   \n\n\n# Model Accuracy using R-2 Score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 6: Fine Tune Model's Hyperparameters using `GridSearchCV()`","metadata":{}},{"cell_type":"code","source":"#Import Libraries\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\n#help(GridSearchCV)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _a- Use GridSearchCV to find the best fitting model_\n\n","metadata":{}},{"cell_type":"code","source":"#from sklearn.metrics import get_scorer_names\n#get_scorer_names()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n\ngrid_search = GridSearchCV(\n    estimator=regressor, # same XGBOOST model which we trained earlier\n    param_grid={\n        \"n_estimators\":[100,200,300, 400, 500], # Number of boosting rounds\n        \"learning_rate\":[0.1,0.2,0.3, 0.4, 0.5], # Step size shrinkage to prevent overfitting\n    },\n    n_jobs = -1, #Number of jobs to run in parallel. -1 means using all processors.\n    scoring=\"r2\",\n    cv=5, # 5-fold cross-validation\n    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n\n# Train hypertuned model \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **NOTE**>  : Process will take ~2.5min to run, in order to accelerate it, increase the size of your Snowflake virtual Warehouse.\n\n","metadata":{}},{"cell_type":"code","source":"# Best Estimator (model) Hyperparameters\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **NOTE**>  : Based on our Grid search results above the best estimator with the lowest R-2 Score hyper-parameters are learning_rate=0.1 and n_estimator=200.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### b- Calculate R-2 Score based on best estimator \n\n","metadata":{}},{"cell_type":"code","source":"# Predict Price using Test set using best estimator (model)\nresult_optimal_model = \n\n# Analyze results using R-2 Score \nprint('Acccuracy of Hypertuned XGBOOST Model:', r2_score(df=result_optimal_model,y_true_col_name=\"PRICE\",y_pred_col_name=\"PREDICTED_PRICE\"))\nprint('Acccuracy of Initial XGBOOST Model:', r2_score(df=result,y_true_col_name=\"PRICE\",y_pred_col_name=\"PREDICTED_PRICE\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 7: Deploy the model as a Vectorized User-Defined Function (UDF)","metadata":{}},{"cell_type":"markdown","source":"> ___**Note:**___> _ When you call `model.predict()` function Snowpark ML creates a temporary UDF, so in order to persist as a permanent UDF, we'll need to define our own UDF.  -- Navigate to Snowflake Query History to see executed queries -- _\n\n","metadata":{}},{"cell_type":"code","source":"#import libraries \nimport joblib\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.snowpark import types as T\nimport snowflake.snowpark.functions as F\nimport cachetools\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _a- Create the end to end pipeline_\n\n","metadata":{}},{"cell_type":"code","source":"# Categorize all the features for modeling\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"]\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}\n\n# Define the end to end pipeline that contains Transforms (preprocessing) and Estimator (optimal XGBRegressor model)\npipe = Pipeline(\n    steps=[\n        (\"OE\", OrdinalEncoder(\n            input_cols= ,  \n            output_cols= , \n            categories = ,  \n            drop_input_cols=True)\n            ),\n        (\"regressor\", XGBRegressor(\n            learning_rate = , # Add Best best_params_ Results here \n            n_estimators = ,  # Add Best best_params_ Results here \n            input_cols=CATEGORICAL_COLUMNS_OE + NUMERICAL_COLUMNS, \n            label_cols=[\"PRICE\"], \n            output_cols=['PREDICTED_PRICE'], \n            n_jobs=-1)\n            )\n    ]\n)\n\n# Random split\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9,0.1], seed =0)\n\n# Train the model and convert it to sklearn pipeline\nxgb_optimal_model = ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b- Save the optimal pipeline within a Snowflake Internal Stage\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn import set_config\nset_config(display=\"diagram\")\nxgb_optimal_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a file name for the model\nMODEL_FILE = 'xgb_optimal_pipe.joblib'\n\n# Serialize and save the model to a file\njoblib.dump(xgb_optimal_model, MODEL_FILE)\n\n# Upload the model file to the specified stage\nsession.file.put(MODEL_FILE, \"@ML_FILES\", overwrite=True, auto_compress=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### c- Register the model as a Vectorized UDF\n\n","metadata":{}},{"cell_type":"markdown","source":"_Vectorized Python UDFs let you define Python functions that receive batches of input rows as Pandas DataFrames and return batches of results as Pandas arrays or Series. You call vectorized Python UDFs the same way you call other Python UDFs.\r\r\nAdvantages of using vectorized Python UDFs compared to the default row-by-row processing pattern include:\r\n\r_\n\n- _The potential for better performance if your Python code operates efficiently on batches of rows.\r_\n- _Less transformation logic is required if you are calling into libraries that operate on Pandas DataFrames or Pandas arrays._\n\n_For more details please visit _[Snowflake Documentation](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-batch)_._\n\n","metadata":{}},{"cell_type":"code","source":"# Define a function to read the model from a file\n@cachetools.cached(cache={})\ndef read_file(filename):\n    import joblib\n    import sys\n    import os\n\n    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n\n    if import_dir:\n        with open(os.path.join(import_dir, filename), 'rb') as file:\n            m = joblib.load(file)\n            return m\n\n\n# Create a vectorized UDF for batch inference\n@F.udf(name=\"predict_diamond_price\",\n        is_permanent=True,\n        stage_location = '@ML_FILES',\n        imports=['@ML_FILES/xgb_optimal_pipe.joblib'],\n        packages=['snowflake-ml-python', 'joblib', 'scikit-learn==1.2.2', 'xgboost==1.7.3', 'cachetools'],\n        replace=True,\n        session=session)\ndef predict_diamond_price(pd_input: T.PandasDataFrame[str, str, str, float, float, float, float, float, float]) -> T.PandasSeries[float]:\n        # Make sure you have the columns in the expected order in the Pandas DataFrame\n    features = [\"CUT\", \"COLOR\", \"CLARITY\", \"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n    pd_input.columns =  \n    model =             \n    prediction =        \n    return              ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### d-Call the UDF and Predict Price for New Diamonds Table\n\n","metadata":{}},{"cell_type":"code","source":"# Create a Snowpark DF containing Diamonds_NEW Table\nnew_diamonds =  \n\n# Apply the UDF to your DataFrame\nnew_diamonds_w_prediction = new_diamonds.with_column(\"PREDICTED_PRICE\", F.call_function(\"predict_diamond_price\", \n                                    F.col(\"CUT\"), F.col(\"COLOR\"), F.col(\"CLARITY\"), \n                                    F.col(\"CARAT\"), F.col(\"DEPTH\"), F.col(\"TABLE_PCT\"), \n                                    F.col(\"X\"), F.col(\"Y\"), F.col(\"Z\"))\n                                    )\n\n# Show the result\nnew_diamonds_w_prediction.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### e-Save Predictions in a  new Snowflake Table \n\n","metadata":{}},{"cell_type":"code","source":"# Write predictions to a Snowflake table\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#session.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ------- END OF THE GUIDED PROJECT.... CONGRATULATIONS !! -------\n\n","metadata":{}}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"James McIntyre","project_id":"71f2594d-0153-4edf-bddb-0244d149f1f8","version":"import","exported_date":"Sat Mar 01 2025 21:02:25 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}